# 🧵 Understanding Goroutines in Go

---

## 🧠 What Are Goroutines?

- Every Go program has at least **one goroutine**: the **main goroutine**, which is automatically created when the process starts.
    
- Simply put, a **goroutine** is a function that runs concurrently (not necessarily in parallel) with other code.
    

---

## 🧑‍💻 Goroutine Example

### Using a Named Function:

```go
func main() {
    go sayHello()  // Start sayHello as a goroutine
    // continue doing other things
}

func sayHello() {
    fmt.Println("hello")
}
```

### Using an Anonymous Function:

```go
go func() {
    fmt.Println("hello")
}()  // immediately invokes the anonymous function
// continue doing other things
```

Alternatively, you can assign the anonymous function to a variable:

```go
sayHello := func() {
    fmt.Println("hello")
}
go sayHello()  // Start the function as a goroutine
// continue doing other things
```

---

## 🛠️ How Goroutines Work

- **Goroutines** are not **OS threads**.
    
- They are **coroutines**: lightweight, non-preemptive concurrent subroutines.
    
    - **Non-preemptive**: Goroutines can't be interrupted by the system.
        
    - Instead, they have **suspension points** where they can yield control and later resume execution.
        

---

## 🔍 Goroutines vs. OS Threads

- **Goroutines** are managed by Go's runtime, not the operating system.
    
- They are more lightweight than **green threads** (threads managed by a language's runtime).
    
- **How many can you create?**
    
    - Go can easily create **millions of goroutines**, as they are much less resource-intensive compared to OS threads.
        

---

## 🚦 Go’s Runtime and Goroutines

- The **Go runtime** automatically manages the scheduling of goroutines.
    
    - **Suspension and resumption** happen when a goroutine becomes **blocked** (e.g., waiting for I/O).
        
    - This makes goroutines **preemptible**, but only at the points where they are blocked.
        
    - The runtime manages this process efficiently, allowing for high concurrency.
        

---

## ⚖️ Concurrency vs. Parallelism

- **Concurrency** means **doing multiple things at the same time**, but **not necessarily in parallel**.
    
- **Goroutines are concurrent** constructs:
    
    - Go schedules and runs multiple goroutines.
        
    - Goroutines give the illusion of parallelism but can run sequentially (in time slices) on a single CPU core.
        
- **Parallelism** requires **multiple processors/cores** to run tasks literally at the same time.
    
    - Goroutines **can be parallel** when run on a multi-core system, but that's not guaranteed.
        

---

## 🧭 Summary

- **Goroutines**: Lightweight concurrent units of execution, not OS threads.
    
- **Coroutines**: Non-preemptive subroutines with automatic suspension and resumption managed by Go's runtime.
    
- **Concurrency**: Achieved through goroutines running in a scheduled manner (may appear parallel on multi-core systems).
    
# 🧵 Go's M:N Scheduler and the Fork-Join Model

---

## 🧠 M:N Scheduling

- Go uses an **M:N scheduler**, meaning it maps **M goroutines (green threads)** to **N OS threads**.
    
- **Goroutines** are scheduled onto **green threads** managed by Go’s runtime.
    
- When there are more goroutines than green threads available:
    
    - The scheduler **distributes goroutines** across available threads.
        
    - **Blocked goroutines** allow other goroutines to run, making the system **efficient** and **responsive**.
        

---

## 🌐 Fork-Join Model

- Go follows the **fork-join concurrency model**.
    

### 1. **Fork**:

- At any point, a program can **fork** or **split off** a new child task (goroutine) to run concurrently with the parent task.
    

### 2. **Join**:

- Eventually, these concurrently running tasks **join** back together at a **join point**, where the parent and child goroutines synchronize.
    

---

## 🚀 Why It Matters

- **M:N Scheduler**: Efficiently handles large numbers of goroutines with fewer OS threads.
    
- **Fork-Join**: Natural for modeling problems where tasks split and then reconvene, making the code easier to reason about.
    

---

## 🔄 In Summary

- **M:N Scheduler**: Distributes goroutines across OS threads efficiently.
    
- **Fork-Join Model**: Lets tasks split and later synchronize, mimicking real-world parallelism scenarios.
    

# 🧵 Deep Dive into Goroutines: Lifecycle, Closures, Memory, and Concurrency in Go

---

## ⚙️ Goroutines Run Concurrently (Not Necessarily in Parallel)

When you run:

```go
go sayHello()
```

You're launching a **concurrent execution branch**, but there's **no guarantee** the `sayHello()` function will run before the main function exits.

> ❗️This creates a race condition, not a proper _join point_. Using `time.Sleep()` only delays termination — it does not ensure synchronization.

---

## 🔄 Closures and Loop Variables

Consider this example:

```go
var wg sync.WaitGroup
salutation := "hello"
wg.Add(1)
go func() {
	defer wg.Done()
	salutation = "welcome"
}()
wg.Wait()
fmt.Println(salutation)
```

- The closure **captures the outer variable `salutation`**, not a copy.
    
- Output may **unexpectedly print** `"welcome"` due to concurrent modification.
    

---

## 🚨 Common Mistake in Loops (Pre-Go 1.24)

```go
for _, salutation := range []string{"hello", "greetings", "good day"} {
	wg.Add(1)
	go func() {
		defer wg.Done()
		fmt.Println(salutation)
	}()
}
```

- Before Go 1.24, this would print:
    

```
good day
good day
good day
```

Because all goroutines captured the **same reference** to `salutation`.

---

## ✅ Correct Way (All Versions of Go)

Pass the loop variable as a function argument:

```go
for _, salutation := range []string{"hello", "greetings", "good day"} {
	wg.Add(1)
	go func(salutation string) {
		defer wg.Done()
		fmt.Println(salutation)
	}(salutation)
}
```

- This creates a **copy of `salutation`** for each goroutine.
    
- Output will now correctly be:
    

```
hello
greetings
good day
```

---

## 🧠 Go 1.24 Improvement

Go 1.24 now **automatically captures loop variables by value** inside closures, making this behavior **safer and more predictable** — though it's still good practice to pass explicitly.

---

## 🪙 Lightweight by Design

- A goroutine starts with just **a few KB of stack memory**.
    
- Stack **grows/shrinks automatically** as needed.
    
- You can create **hundreds of thousands** of goroutines efficiently.
    

---

### 🧪 Benchmark: How Much Memory Does a Goroutine Use?

```go
memConsumed := func() uint64 {
	runtime.GC()
	var s runtime.MemStats
	runtime.ReadMemStats(&s)
	return s.Sys
}

var c <-chan interface{}
var wg sync.WaitGroup
noop := func() { wg.Done(); <-c }

const numGoroutines = 1e4
wg.Add(numGoroutines)
before := memConsumed()

for i := numGoroutines; i > 0; i-- {
	go noop()
}

wg.Wait()
after := memConsumed()

fmt.Printf("%.3fkb", float64(after-before)/numGoroutines/1000)
```

**Result:**

```
2.817kb per goroutine
```

---

## 🧼 Note on Garbage Collection

- Goroutines are **not garbage collected**.
    
- A goroutine that **never exits** will remain in memory until the **entire process exits**.
    

```go
go func() {
	// Block forever
	select {}
}()
```

---

## 🧠 Final Takeaways

|Feature|Insight|
|---|---|
|🔁 Goroutines|Run concurrently, not guaranteed to run before `main` exits|
|🔍 Closures|Capture variables by reference (unless passed as params or in Go 1.24+)|
|🧵 Scheduling|M:N scheduler efficiently maps goroutines to threads|
|🪶 Memory|~2.8KB per goroutine, scalable to millions|
|🚫 GC Behavior|Goroutines are not auto-collected; manage lifecycle carefully|

I'm glad you liked the explanation! Here's the **Amdahl's Law tutorial** in a structured format for you to save and refer to later:

---

## 📚 **Amdahl's Law and Its Connection with Go** 🚀

### **1. What is Amdahl's Law?**

**Amdahl's Law** is a principle in parallel computing that shows the potential speedup of a program when only part of it can be parallelized. The law helps us understand the **diminishing returns** of parallelizing a program as the **sequential part** of the program becomes more significant.

### **Mathematical Formula:**

Amdahl's Law is expressed mathematically as:

Sspeedup=1(1−P)+PNS_{\text{speedup}} = \frac{1}{(1 - P) + \frac{P}{N}}

Where:

- SspeedupS_{\text{speedup}} is the overall speedup of the program when NN processors are used.
    
- PP is the **parallelizable portion** of the program (between 0 and 1).
    
- NN is the number of processors used.
    

---

### **2. How Amdahl's Law Works:**

- **If P=1P = 1**: The entire program can be parallelized, and adding more processors leads to **linear speedup** (perfect scaling).
    
- **If P=0P = 0**: The program is entirely sequential, and adding processors won’t help improve performance.
    
- **If PP is less than 1** (a realistic scenario): Even as you add more processors, the speedup will have diminishing returns, because the **sequential part** of the program remains a bottleneck.
    

---

### **3. Amdahl's Law with Examples:**

#### Example 1: 80% Parallelizable

Let’s assume that **80% of a program** can be parallelized, so P=0.8P = 0.8, and the rest is sequential. If we add more processors:

- **With 2 processors:**
    

Sspeedup=1(1−0.8)+0.82=10.2+0.4=10.6=1.67S_{\text{speedup}} = \frac{1}{(1 - 0.8) + \frac{0.8}{2}} = \frac{1}{0.2 + 0.4} = \frac{1}{0.6} = 1.67

You get a **1.67x speedup**, less than double the performance.

- **With 10 processors:**
    

Sspeedup=1(1−0.8)+0.810=10.2+0.08=10.28=3.57S_{\text{speedup}} = \frac{1}{(1 - 0.8) + \frac{0.8}{10}} = \frac{1}{0.2 + 0.08} = \frac{1}{0.28} = 3.57

Even with 10 processors, you only get a **3.57x speedup**, much less than the **10x** you'd expect if the program were fully parallelizable.

---

### **4. Why Amdahl's Law is Important:**

Amdahl’s law helps us understand the limitations of **parallel computing** and why adding more processors might not always result in proportional speedup. If a significant portion of a program can’t be parallelized, the performance benefits from parallelism will be limited.

---

### **5. "Not Limited by Amdahl's Law" in Go:**

When the statement says **"not limited by Amdahl's law"**, it means that your problem can be **fully parallelized** or can scale efficiently with the number of processors, so Amdahl’s law doesn’t impose a significant constraint.

---

### **6. How Amdahl’s Law Relates to Go:**

In Go, **goroutines** provide a powerful mechanism for concurrency. Go is designed to allow you to launch many goroutines concurrently with very little overhead. This is especially useful for problems that can be **parallelized**, allowing the program to scale effectively with more processors.

- **Goroutines** are lightweight compared to traditional threads, and they are managed by Go's runtime, which can schedule them efficiently across available CPU cores.
    
- **If the problem can be fully parallelized**, you can use multiple processors effectively, and your program will **scale well** with the number of processors, without hitting the diminishing returns predicted by Amdahl’s law.
    

---

### **7. When Amdahl’s Law is a Factor:**

Amdahl’s law becomes relevant when a program has a **non-negligible sequential portion**. In such cases, no matter how many processors you add, the **sequential part** of the program will always limit the overall speedup.

#### Example: Program with 10% Sequential Code

Let’s assume 10% of a program is sequential, so P=0.9P = 0.9 (90% parallelizable), and the rest is sequential:

- **With 10 processors:**
    

Sspeedup=1(1−0.9)+0.910=10.1+0.09=10.19=5.26S_{\text{speedup}} = \frac{1}{(1 - 0.9) + \frac{0.9}{10}} = \frac{1}{0.1 + 0.09} = \frac{1}{0.19} = 5.26

In this case, even with 10 processors, you only get a **5.26x speedup**, much less than the **10x** you'd expect if the program were fully parallelizable.

---

### **8. Conclusion and Practical Application in Go:**

In Go, the ability to launch **goroutines** makes **concurrency** much simpler and more efficient compared to traditional threads. However, you need to be aware of Amdahl’s law when designing highly concurrent programs:

- **If your problem has a significant sequential component**, adding more goroutines or processors will provide diminishing returns as predicted by Amdahl’s law.
    
- **If your problem is highly parallelizable**, Go’s goroutines can scale well with the number of processors, and you’ll see a much larger performance boost.
    

**Go and Goroutines** offer a great opportunity to leverage concurrency in an efficient and intuitive way, but like any tool, understanding its limitations is key to making the best use of it!

---

### **Key Takeaways:**

- **Amdahl's Law** explains the limits of parallelism in a program and how the sequential portion will always be a bottleneck.
    
- **Go’s goroutines** provide a simple way to scale a program with multiple processors, but only if the problem allows for significant parallelism.
    
- If your problem is **not constrained by Amdahl’s law**, you can expect significant **scalability** in Go when using more goroutines and processors.
    



# ⚙️ Goroutines, Context Switching, and `sync.WaitGroup` in Go

---

## 🔄 Context Switching: OS Threads vs. Goroutines

- **Context switching** occurs when a system pauses one concurrent task to switch to another.  
    In OS threads, it's expensive:
    
    - Saves/restores register values, memory maps, and thread state.
        
    - Consumes CPU time without doing productive work.
        
- **In Go**, context switching is handled by the **runtime scheduler** using an **M:N model**:
    
    - **M** goroutines are mapped onto **N** OS threads.
        
    - When one goroutine blocks, another is scheduled in its place.
        

---

### 🧪 Benchmark: OS Thread Context Switch

```bash
taskset -c 0 perf bench sched pipe -T
```

Result:

```
2.935784 μs/op → 1.467 μs per switch
```

---

### 🧪 Benchmark: Goroutine Context Switch

```go
BenchmarkContextSwitch    5000000    225 ns/op
```

- **Goroutine switching** is ~92% faster than OS threads.
    
- Go's runtime optimizes what to persist and when, making switching lightweight.
    

---

## 🧵 Lightweight Nature of Goroutines

- Each goroutine starts with just a **few KB** of stack memory.
    
- The stack **grows/shrinks dynamically**.
    
- You can easily create **millions** of goroutines in a modern system.
    

```go
const numGoroutines = 1e4
fmt.Printf("%.3fkb", float64(after-before)/numGoroutines/1000)
```

Example Output:

```
2.817kb per goroutine
```

---

## 🧹 Caveat: Goroutines Are Not Garbage Collected

```go
go func() {
	// blocks forever
	select {}
}()
```

- Abandoned goroutines **stay alive** until the process exits.
    
- Make sure you manage goroutines intentionally.
    

---

## ⏳ Using `sync.WaitGroup` for Synchronization

The `sync.WaitGroup` is used to **wait for multiple goroutines** to finish.

### 🔹 Example:

```go
var wg sync.WaitGroup

wg.Add(1)
go func() {
	defer wg.Done()
	fmt.Println("1st goroutine sleeping...")
	time.Sleep(1 * time.Second)
}()

wg.Add(1)
go func() {
	defer wg.Done()
	fmt.Println("2nd goroutine sleeping...")
	time.Sleep(2 * time.Second)
}()

wg.Wait()
fmt.Println("All goroutines complete.")
```

Output:

```
2nd goroutine sleeping...
1st goroutine sleeping...
All goroutines complete.
```

---

## 📊 How `WaitGroup` Works

- `Add(n)`: Adds `n` to the internal counter.
    
- `Done()`: Decrements the counter by 1.
    
- `Wait()`: Blocks until the counter hits **0**.
    

> ⚠️ Always call `Add()` **before** launching the goroutines to avoid race conditions.

---

### 🔁 Launching Multiple Goroutines with `WaitGroup`

```go
hello := func(wg *sync.WaitGroup, id int) {
	defer wg.Done()
	fmt.Printf("Hello from %v!\n", id)
}

const numGreeters = 5
var wg sync.WaitGroup

wg.Add(numGreeters)
for i := 0; i < numGreeters; i++ {
	go hello(&wg, i+1)
}

wg.Wait()
```

Sample Output:

```
Hello from 3!
Hello from 5!
Hello from 1!
Hello from 4!
Hello from 2!
```

---

## ✅ Summary

|Concept|Key Insight|
|---|---|
|**Context Switching**|Cheaper in goroutines than OS threads (~225ns vs. ~1.4μs)|
|**Goroutines**|Lightweight, scalable, and managed by Go's runtime|
|**WaitGroup**|Synchronizes multiple goroutines, blocking until all have completed|
|**Best Practice**|Always call `Add()` before launching goroutines to avoid race conditions|
|**Memory Footprint**|~2.8 KB per goroutine, making massive concurrency feasible|

# 🔐 `sync.Mutex` vs `sync.RWMutex` in Go — Managing Critical Sections

---

## 🔒 What Is a `Mutex`?

- **`Mutex`** stands for **Mutual Exclusion**.
    
- It allows only **one goroutine at a time** to access a critical section — a piece of code that **reads/writes shared data**.
    

### 🧑‍💻 Example:

```go
var count int
var lock sync.Mutex

increment := func() {
	lock.Lock()
	defer lock.Unlock()
	count++
	fmt.Printf("Incrementing: %d\n", count)
}

decrement := func() {
	lock.Lock()
	defer lock.Unlock()
	count--
	fmt.Printf("Decrementing: %d\n", count)
}

var wg sync.WaitGroup
for i := 0; i <= 5; i++ {
	wg.Add(1)
	go func() {
		defer wg.Done()
		increment()
	}()
}
for i := 0; i <= 5; i++ {
	wg.Add(1)
	go func() {
		defer wg.Done()
		decrement()
	}()
}
wg.Wait()
fmt.Println("Arithmetic complete.")
```

- `Lock()` ensures **exclusive access** to `count`.
    
- `Unlock()` (with `defer`) guarantees that lock will be released, even on panic.
    
- Result: Safe increment/decrement, but not necessarily in order.
    

---

## 🔁 Performance Cost of Critical Sections

- Entering/exiting a critical section guarded by `Mutex` is **non-trivial**.
    
- Goal: **Minimize time spent** inside it.
    

---

## 📖 Introducing `sync.RWMutex`

- `RWMutex` allows **multiple readers** or **one writer** — not both at once.
    
- Use `RLock()`/`RUnlock()` for read-only access.
    
- Use `Lock()`/`Unlock()` for write access.
    

---

## 🧪 Benchmark: Comparing `Mutex` vs `RWMutex`

### 🔬 Benchmark Setup

```go
producer := func(wg *sync.WaitGroup, l sync.Locker) {
	defer wg.Done()
	for i := 5; i > 0; i-- {
		l.Lock()
		l.Unlock()
		time.Sleep(1 * time.Second)
	}
}

observer := func(wg *sync.WaitGroup, l sync.Locker) {
	defer wg.Done()
	l.Lock()
	defer l.Unlock()
}

test := func(count int, mutex, rwMutex sync.Locker) time.Duration {
	var wg sync.WaitGroup
	wg.Add(count + 1)
	start := time.Now()
	go producer(&wg, mutex)
	for i := count; i > 0; i-- {
		go observer(&wg, rwMutex)
	}
	wg.Wait()
	return time.Since(start)
}
```

### 📊 Output Summary

|Readers|RWMutex|Mutex|
|---|---|---|
|1|38.343μs|15.854μs|
|2|21.86μs|13.2μs|
|4|31.01μs|31.358μs|
|64|141.708μs|163.43μs|
|1024|459.349μs|850.601μs|
|8192|2.167814ms|4.13665ms|
|65536|16.767161ms|30.948295ms|
|524288|303.865661ms|231.072729ms|

### 📌 Observation

- **At low reader counts**, `Mutex` can be faster due to lower coordination overhead.
    
- **At higher reader counts**, `RWMutex` shows better performance because **multiple readers can proceed simultaneously**.
    

---

## 📎 Note

> As seen in the benchmark, **performance varies considerably**. In some cases, reader-lock (`RWMutex`) outperforms a full lock (`Mutex`), and in others, a basic lock performs better.  
> **Generally, when you have many readers**, `RWMutex` _tends_ to offer better performance — **but not always**.

---

## ✅ Summary

|Feature|`Mutex`|`RWMutex`|
|---|---|---|
|Access|One goroutine at a time|Many readers or one writer|
|Best for|Read/write access|Mostly-read scenarios|
|Performance|Faster with few goroutines|Better with many readers|
|Lock Methods|`Lock()`, `Unlock()`|`Lock()`, `Unlock()`, `RLock()`, `RUnlock()`|

---
